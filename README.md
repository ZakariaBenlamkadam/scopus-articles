# ğŸš€ Project: Scholarly Insights Explorer! ğŸ“šğŸ”

Welcome aboard! This project is your trusty guide to navigating the vast world of academic publications, using data from Scopus. ğŸ“œâœ¨ We've built a data pipeline that takes raw research data, cleans it up, and structures it for insightful analysis. Let's dive in! ğŸŒŠ

## ğŸ¯ Project Goals

*   Extract and process data from Scopus to gain meaningful insights. ğŸ§
*   Create a scalable and robust data pipeline. ğŸ’ª
*   Structure the data for efficient querying and reporting. ğŸ“Š
*   Uncover hidden patterns and trends in academic research. ğŸ•µï¸â€â™€ï¸

## âš™ï¸ Project Architecture

Our pipeline is like a well-oiled machine! Here's a high-level overview:

![Architecture](bi.png) 

**Key Stages:**

1.  **Sourcing:** ğŸ“¦ Data is extracted from Scopus in CSV format.
2.  **Storage:** â˜ï¸ Raw data is safely stored in Azure Blob Storage.
3.  **Transformation:** âœ¨ PySpark in Databricks is used to clean, transform, and structure the data.
4.  **Data Warehousing:** ğŸ’¾ Transformed data is loaded into Snowflake for secure and scalable storage.
5.  **Dashboarding:** ğŸ“Š Power BI brings the data to life through visualizations and dashboards.

## ğŸ—‚ï¸ Data Modeling

The transformed data is structured into a star schema, optimized for analysis. Here's the layout:

![Modeling](modeling.jpg) 


**Core Tables:**

*   **`ARTICLES` (Fact Table):** Contains general information about publications, like titles, abstracts, citation counts, and more. ğŸ“š
*   **`KEYWORDS` (Dimension Table):** Lists the keywords associated with each article. ğŸ”‘
*   **`AUTHORS` (Dimension Table):** Contains names and IDs of the authors of the articles. âœï¸
*   **`AFFILIATIONS` (Dimension Table):** Stores author affiliations, including the institution, city, and country. ğŸ¢ğŸ—ºï¸

**Relationships:**

*   All dimension tables relate back to the `ARTICLES` table using one-to-many relationships. ğŸ”—

## ğŸ› ï¸ Technologies Used

*   **Data Sources:** Scopus (CSV files) ğŸ“œ
*   **Cloud Storage:** Azure Blob Storage â˜ï¸
*   **Data Processing:** Databricks with PySpark âœ¨
*   **Data Warehousing:** Snowflake â„ï¸
*   **Data Visualization:** Power BI ğŸ“Š

## ğŸ“ Transformation Steps

Our PySpark script is a data wrangling masterpiece! Here are some key steps:

*   **Data Type Casting:** Ensuring data columns have the correct type. ğŸ§™â€â™‚ï¸
*   **Data Cleaning:** Removing invalid characters and standardizing strings. ğŸ§¹
*   **Data Enrichment:** Adding useful columns by splitting and combining existing data. ğŸ§¬
*   **Table Splitting:** Decomposing the original data into four related tables for a relational structure. ğŸ§©

## ğŸš€ Let's Dive into the Code!

For detailed code, check out our PySpark transformation script. It is where all the magic happens! âœ¨
It takes data from Scopus and creates four different tables, handling null values and splitting different fields into arrays.

## ğŸ’¡ Potential Use Cases

*   Uncovering trends in academic research. ğŸ“ˆ
*   Tracking the impact of different publications. ğŸ“Š
*   Analyzing collaboration networks between authors. ğŸ¤
*   Exploring research topics and keywords. ğŸ”

## ğŸ‰ Conclusion

We hope you find our Scholarly Insights Explorer fun and useful! ğŸš€ Let us know your feedback and happy exploring! ğŸ‰

Feel free to reach out if you have any questions or want to know more about the project! ğŸ’¬
